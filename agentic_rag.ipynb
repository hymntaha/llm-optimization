{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b1d9ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from typing import List, Dict\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.graph.message import MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from IPython.display import Image, display\n",
    "from tavily import TavilyClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9ee71ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0992f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_fn = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c51ba000",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File path compact-guide-to-large-language-models.pdf is not a valid file or url",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m vector_store = Chroma(\n\u001b[32m      2\u001b[39m     collection_name=\u001b[33m\"\u001b[39m\u001b[33mudacity\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     embedding_function=embeddings_fn\n\u001b[32m      4\u001b[39m )\n\u001b[32m      6\u001b[39m file_path = \u001b[33m\"\u001b[39m\u001b[33mcompact-guide-to-large-language-models.pdf\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m loader = \u001b[43mPyPDFLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m pages = []\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m loader.alazy_load():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.6/lib/python3.12/site-packages/langchain_community/document_loaders/pdf.py:281\u001b[39m, in \u001b[36mPyPDFLoader.__init__\u001b[39m\u001b[34m(self, file_path, password, headers, extract_images, mode, images_parser, images_inner_format, pages_delimiter, extraction_mode, extraction_kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    239\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    240\u001b[39m     file_path: Union[\u001b[38;5;28mstr\u001b[39m, PurePath],\n\u001b[32m   (...)\u001b[39m\u001b[32m    250\u001b[39m     extraction_kwargs: Optional[\u001b[38;5;28mdict\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    251\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    252\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Initialize with a file path.\u001b[39;00m\n\u001b[32m    253\u001b[39m \n\u001b[32m    254\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    279\u001b[39m \u001b[33;03m        `aload` methods to retrieve parsed documents with content and metadata.\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28mself\u001b[39m.parser = PyPDFParser(\n\u001b[32m    283\u001b[39m         password=password,\n\u001b[32m    284\u001b[39m         mode=mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    290\u001b[39m         extraction_kwargs=extraction_kwargs,\n\u001b[32m    291\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.6/lib/python3.12/site-packages/langchain_community/document_loaders/pdf.py:140\u001b[39m, in \u001b[36mBasePDFLoader.__init__\u001b[39m\u001b[34m(self, file_path, headers)\u001b[39m\n\u001b[32m    138\u001b[39m         \u001b[38;5;28mself\u001b[39m.file_path = \u001b[38;5;28mstr\u001b[39m(temp_pdf)\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(\u001b[38;5;28mself\u001b[39m.file_path):\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mFile path \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m is not a valid file or url\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mself\u001b[39m.file_path)\n",
      "\u001b[31mValueError\u001b[39m: File path compact-guide-to-large-language-models.pdf is not a valid file or url"
     ]
    }
   ],
   "source": [
    "vector_store = Chroma(\n",
    "    collection_name=\"udacity\",\n",
    "    embedding_function=embeddings_fn\n",
    ")\n",
    "\n",
    "file_path = \"compact-guide-to-large-language-models.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = []\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(pages)\n",
    "\n",
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e796fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05dcf26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(MessagesState):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    search_required: str = \"NO\"\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d029479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State):\n",
    "    question = state[\"question\"]\n",
    "    retrieved_docs = vector_store.similarity_search(question)\n",
    "    return {\"documents\": retrieved_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f29b03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(state: State):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    \n",
    "    template = ChatPromptTemplate([\n",
    "        (\"system\", \"You are an assistant for evaluating context.\"),\n",
    "        (\"human\", \"Use the following pieces of retrieved context to determine if an \" \n",
    "                  \"additional search is required. If the context is relevant to the \" \n",
    "                  \"question, your response should be 'NO', i.e. no need for search. \"\n",
    "                  \"If the context is not relevant, say 'YES', i.e. search the web for an answer. \" \n",
    "                  \"Don't explain anything, if it's to search the web, say YES, otherwise, NO. \"\n",
    "                  \"\\n# Question: \\n-> {question} \"\n",
    "                  \"\\n# Context: \\n-> {context} \"\n",
    "                  \"\\n# Answer: \"),\n",
    "    ])\n",
    "\n",
    "    chain = template | llm | StrOutputParser()\n",
    "\n",
    "    search_required = chain.invoke(\n",
    "        {\"context\": docs_content, \"question\": question}\n",
    "    )\n",
    "\n",
    "    return {\"search_required\": search_required}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ef6e7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def web_search(question:str)->Dict:\n",
    "    \"\"\"\n",
    "    Return top search results for a given search query\n",
    "    \"\"\"\n",
    "    tavily_client = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "    response = tavily_client.search(question)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f88a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools([web_search])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2af7a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def researcher(state: State):\n",
    "    question = state[\"question\"]\n",
    "    messages = state[\"messages\"]\n",
    "    if not messages:\n",
    "        human_message = HumanMessage(\n",
    "            \"Conduct a web research to findout the answer to the following question: \"\n",
    "            f\"```{question}```\"\n",
    "        )\n",
    "        messages = [human_message]\n",
    "    \n",
    "    ai_message = llm_with_tools.invoke(messages)\n",
    "    messages.append(ai_message)\n",
    "    return {\"messages\": messages, \"answer\": ai_message.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16a9cc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(state: State):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    \n",
    "    template = ChatPromptTemplate([\n",
    "        (\"system\", \"You are an assistant for question-answering tasks.\"),\n",
    "        (\"human\", \"Use the following pieces of retrieved context to answer the question. \"\n",
    "                \"If you don't know the answer, just say that you don't know. \" \n",
    "                \"Use three sentences maximum and keep the answer concise. \"\n",
    "                \"\\n# Question: \\n-> {question} \"\n",
    "                \"\\n# Context: \\n-> {context} \"\n",
    "                \"\\n# Answer: \"),\n",
    "    ])\n",
    "\n",
    "    messages = template.invoke(\n",
    "        {\"context\": docs_content, \"question\": question}\n",
    "    ).to_messages()\n",
    "\n",
    "    return {\"messages\": messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae4063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state: State):\n",
    "    ai_message = llm.invoke(state[\"messages\"])\n",
    "    return {\"answer\": ai_message.content, \"messages\": ai_message}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb2612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_router(state: MessagesState):\n",
    "    search_required = state[\"search_required\"]\n",
    "    if search_required.lower() == \"yes\":\n",
    "        return \"researcher\"\n",
    "\n",
    "    return \"augment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e79a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_router(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"web_search\"\n",
    "\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b9f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)\n",
    "\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"augment\", augment)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"evaluator\", evaluator)\n",
    "workflow.add_node(\"researcher\", researcher)\n",
    "workflow.add_node(\"web_search\", ToolNode([web_search]))\n",
    "\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"evaluator\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    source=\"evaluator\", \n",
    "    path=rag_router, \n",
    "    path_map=[\"augment\", \"researcher\"]\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    source=\"researcher\", \n",
    "    path=tool_router, \n",
    "    path_map=[\"web_search\", END]\n",
    ")\n",
    "workflow.add_edge(\"web_search\", \"researcher\")\n",
    "\n",
    "workflow.add_edge(\"augment\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e28f0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = workflow.compile()\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b500ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = graph.invoke(\n",
    "    input={\"question\": \"What is Pokemon?\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f01f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83d1de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in output[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce36e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"search_required\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e874e1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = graph.invoke(\n",
    "    input={\"question\": \"What is Open Source model?\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b173c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"search_required\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e350a28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in output[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
